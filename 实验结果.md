# 实验结果
## 第一第二问
### 正常预测（完整）
```bash
Image: picture\bowl-3243264_1280.jpg
Clean:
Top 1: mortar (class 666), prob = 0.6272
Top 2: soup bowl (class 809), prob = 0.1711
Top 3: mixing bowl (class 659), prob = 0.1615
Top 4: ladle (class 618), prob = 0.0149
Top 5: wok (class 909), prob = 0.0083
Image: picture\butterflies-4327296_1280.jpg
Clean:
Top 1: monarch (class 323), prob = 0.2976
Top 2: ringlet (class 322), prob = 0.2515
Top 3: lycaenid (class 326), prob = 0.2482
Top 4: sulphur butterfly (class 325), prob = 0.0684
Top 5: lacewing (class 318), prob = 0.0672
Image: picture\camera-2169435_1280.jpg
Clean:
Top 1: reflex camera (class 759), prob = 0.9652
Top 2: lens cap (class 622), prob = 0.0295
Top 3: Polaroid camera (class 732), prob = 0.0052
Top 4: tripod (class 872), prob = 0.0001
Top 5: projector (class 745), prob = 0.0000
Image: picture\chartres-cathedral-1021517_1280.jpg
Clean:
Top 1: bell cote (class 442), prob = 0.5218
Top 2: church (class 497), prob = 0.4360
Top 3: analog clock (class 409), prob = 0.0293
Top 4: monastery (class 663), prob = 0.0034
Top 5: wall clock (class 892), prob = 0.0032
Image: picture\dandelion-851614_1280.jpg
Clean:
Top 1: bee (class 309), prob = 0.9984
Top 2: fly (class 308), prob = 0.0010
Top 3: apiary (class 410), prob = 0.0002
Top 4: honeycomb (class 599), prob = 0.0002
Top 5: ant (class 310), prob = 0.0001
Image: picture\elephant-1562127_1280.jpg
Clean:
Top 1: African elephant (class 386), prob = 0.5208
Top 2: tusker (class 101), prob = 0.4770
Top 3: Indian elephant (class 385), prob = 0.0020
Top 4: warthog (class 343), prob = 0.0000
Top 5: zebra (class 340), prob = 0.0000
Image: picture\example.jpg
Clean:
Top 1: giant panda (class 388), prob = 0.9996
Top 2: soccer ball (class 805), prob = 0.0001
Top 3: lesser panda (class 387), prob = 0.0001
Top 4: indri (class 384), prob = 0.0000
Top 5: dalmatian (class 251), prob = 0.0000
Image: picture\horse-769128_1280.jpg
Clean:
Top 1: sorrel (class 339), prob = 0.9529
Top 2: Arabian camel (class 354), prob = 0.0257
Top 3: hartebeest (class 351), prob = 0.0040
Top 4: Rhodesian ridgeback (class 159), prob = 0.0031
Top 5: boxer (class 242), prob = 0.0020
Image: picture\nature-3084703_1280.jpg
Clean:
Top 1: cock (class 7), prob = 0.9899
Top 2: hen (class 8), prob = 0.0101
Top 3: stinkhorn (class 994), prob = 0.0000
Top 4: partridge (class 86), prob = 0.0000
Top 5: crane bird (class 134), prob = 0.0000
Image: picture\nature-3358233_1280.jpg
Clean:
Top 1: rapeseed (class 984), prob = 1.0000
Top 2: wing (class 908), prob = 0.0000
Top 3: school bus (class 779), prob = 0.0000
Top 4: sunglasses (class 837), prob = 0.0000
Top 5: parachute (class 701), prob = 0.0000
Image: picture\pexels-dreamypixel-547115.jpg
Clean:
Top 1: alp (class 970), prob = 0.9043
Top 2: valley (class 979), prob = 0.0221
Top 3: mountain tent (class 672), prob = 0.0195
Top 4: tripod (class 872), prob = 0.0080
Top 5: volcano (class 980), prob = 0.0066
Image: picture\rabbits-2174679_1280.jpg
Clean:
Top 1: Persian cat (class 283), prob = 0.3123
Top 2: Angora (class 332), prob = 0.2326
Top 3: hare (class 331), prob = 0.0628
Top 4: corn (class 987), prob = 0.0297
Top 5: hamster (class 333), prob = 0.0280
Image: picture\shanghai-1029368_1280.jpg
Clean:
Top 1: water tower (class 900), prob = 0.6864
Top 2: airship (class 405), prob = 0.0552
Top 3: cab (class 468), prob = 0.0185
Top 4: crane (class 517), prob = 0.0120
Top 5: hair spray (class 585), prob = 0.0118
Image: picture\violin-924349_1280.jpg
Clean:
Top 1: violin (class 889), prob = 0.7175
Top 2: cello (class 486), prob = 0.2792
Top 3: acoustic guitar (class 402), prob = 0.0031
Top 4: banjo (class 420), prob = 0.0001
Top 5: electric guitar (class 546), prob = 0.0000
Image: picture\中性笔.jpg
Clean:
Top 1: ballpoint (class 418), prob = 0.9423
Top 2: fountain pen (class 563), prob = 0.0132
Top 3: syringe (class 845), prob = 0.0106
Top 4: microphone (class 650), prob = 0.0089
Top 5: screwdriver (class 784), prob = 0.0036
Image: picture\剪刀.jpg
Clean:
Top 1: can opener (class 473), prob = 0.8550
Top 2: corkscrew (class 512), prob = 0.0511
Top 3: hook (class 600), prob = 0.0217
Top 4: letter opener (class 623), prob = 0.0211
Top 5: stethoscope (class 823), prob = 0.0080
Image: picture\卷心菜（甘蓝）.jpg
Clean:
Top 1: head cabbage (class 936), prob = 0.9992
Top 2: cauliflower (class 938), prob = 0.0004
Top 3: artichoke (class 944), prob = 0.0001
Top 4: shower cap (class 793), prob = 0.0001
Top 5: cucumber (class 943), prob = 0.0000
Image: picture\台式电脑1.jpg
Clean:
Top 1: desktop computer (class 527), prob = 0.9557
Top 2: screen (class 782), prob = 0.0178
Top 3: monitor (class 664), prob = 0.0081
Top 4: notebook (class 681), prob = 0.0074
Top 5: mouse (class 673), prob = 0.0031
Image: picture\大熊猫.jpg
Clean:
Top 1: giant panda (class 388), prob = 0.9896
Top 2: soccer ball (class 805), prob = 0.0014
Top 3: American black bear (class 295), prob = 0.0011
Top 4: Newfoundland (class 256), prob = 0.0008
Top 5: brown bear (class 294), prob = 0.0005
Image: picture\打火机.jpg
Clean:
Top 1: lighter (class 626), prob = 1.0000
Top 2: pencil sharpener (class 710), prob = 0.0000
Top 3: whistle (class 902), prob = 0.0000
Top 4: screwdriver (class 784), prob = 0.0000
Top 5: switch (class 844), prob = 0.0000
Image: picture\橘子（橙子）.jpg
Clean:
Top 1: orange (class 950), prob = 0.9937
Top 2: lemon (class 951), prob = 0.0046
Top 3: banana (class 954), prob = 0.0007
Top 4: pomegranate (class 957), prob = 0.0004
Top 5: croquet ball (class 522), prob = 0.0001
Image: picture\水杯.jpg
Clean:
Top 1: coffee mug (class 504), prob = 0.6944
Top 2: cup (class 968), prob = 0.2250
Top 3: pitcher (class 725), prob = 0.0369
Top 4: water jug (class 899), prob = 0.0070
Top 5: measuring cup (class 647), prob = 0.0060
Image: picture\洗衣机.jpg
Clean:
Top 1: washer (class 897), prob = 0.9978
Top 2: loudspeaker (class 632), prob = 0.0006
Top 3: radio (class 754), prob = 0.0004
Top 4: dishwasher (class 534), prob = 0.0003
Top 5: microwave (class 651), prob = 0.0001
Image: picture\火.jpg
Clean:
Top 1: torch (class 862), prob = 0.8531
Top 2: fire screen (class 556), prob = 0.1101
Top 3: stove (class 827), prob = 0.0114
Top 4: matchstick (class 644), prob = 0.0061
Top 5: volcano (class 980), prob = 0.0053
Image: picture\牛.jpg
Clean:
Top 1: ox (class 345), prob = 0.9816
Top 2: oxcart (class 690), prob = 0.0105
Top 3: plow (class 730), prob = 0.0050
Top 4: sorrel (class 339), prob = 0.0015
Top 5: ram (class 348), prob = 0.0002
Image: picture\狗.jpg
Clean:
Top 1: Labrador retriever (class 208), prob = 0.7736
Top 2: golden retriever (class 207), prob = 0.0344
Top 3: Great Pyrenees (class 257), prob = 0.0246
Top 4: clumber (class 216), prob = 0.0245
Top 5: kuvasz (class 222), prob = 0.0227
Image: picture\猪.jpg
Clean:
Top 1: hog (class 341), prob = 0.5649
Top 2: piggy bank (class 719), prob = 0.1800
Top 3: guinea pig (class 338), prob = 0.1124
Top 4: wild boar (class 342), prob = 0.0147
Top 5: Chihuahua (class 151), prob = 0.0133
Image: picture\猫.jpg
Clean:
Top 1: Egyptian cat (class 285), prob = 0.7088
Top 2: tabby (class 281), prob = 0.1159
Top 3: tiger cat (class 282), prob = 0.0932
Top 4: lynx (class 287), prob = 0.0206
Top 5: Persian cat (class 283), prob = 0.0178
Image: picture\电池.jpg
Clean:
Top 1: lighter (class 626), prob = 0.6243
Top 2: pencil sharpener (class 710), prob = 0.3664
Top 3: oil filter (class 686), prob = 0.0064
Top 4: whistle (class 902), prob = 0.0010
Top 5: binoculars (class 447), prob = 0.0003
Image: picture\百香果.jpg
Clean:
Top 1: pomegranate (class 957), prob = 0.8295
Top 2: fig (class 952), prob = 0.0539
Top 3: buckeye (class 990), prob = 0.0421
Top 4: orange (class 950), prob = 0.0128
Top 5: custard apple (class 956), prob = 0.0093
Image: picture\相机.jpg
Clean:
Top 1: reflex camera (class 759), prob = 0.9909
Top 2: lens cap (class 622), prob = 0.0088
Top 3: tripod (class 872), prob = 0.0001
Top 4: binoculars (class 447), prob = 0.0000
Top 5: projector (class 745), prob = 0.0000
Image: picture\篮球.jpg
Clean:
Top 1: basketball (class 430), prob = 0.9988
Top 2: volleyball (class 890), prob = 0.0007
Top 3: croquet ball (class 522), prob = 0.0002
Top 4: rugby ball (class 768), prob = 0.0001
Top 5: soccer ball (class 805), prob = 0.0000
Image: picture\绵羊.jpg
Clean:
Top 1: ram (class 348), prob = 0.9965
Top 2: bighorn (class 349), prob = 0.0027
Top 3: llama (class 355), prob = 0.0004
Top 4: kuvasz (class 222), prob = 0.0001
Top 5: Border collie (class 232), prob = 0.0000
Image: picture\耳机.jpg
Clean:
Top 1: hook (class 600), prob = 0.6150
Top 2: microphone (class 650), prob = 0.0580
Top 3: dumbbell (class 543), prob = 0.0471
Top 4: loudspeaker (class 632), prob = 0.0366
Top 5: coil (class 506), prob = 0.0306
Image: picture\自行车.jpg
Clean:
Top 1: mountain bike (class 671), prob = 0.9675
Top 2: bicycle-built-for-two (class 444), prob = 0.0162
Top 3: disk brake (class 535), prob = 0.0074
Top 4: unicycle (class 880), prob = 0.0045
Top 5: tricycle (class 870), prob = 0.0017
Image: picture\苹果.jpg
Clean:
Top 1: Granny Smith (class 948), prob = 0.6487
Top 2: strawberry (class 949), prob = 0.1327
Top 3: fig (class 952), prob = 0.0895
Top 4: pomegranate (class 957), prob = 0.0398
Top 5: orange (class 950), prob = 0.0208
Image: picture\荔枝.jpg
Clean:
Top 1: strawberry (class 949), prob = 0.8768
Top 2: cup (class 968), prob = 0.0234
Top 3: trifle (class 927), prob = 0.0198
Top 4: pot (class 738), prob = 0.0109
Top 5: mixing bowl (class 659), prob = 0.0080
Image: picture\菜花.jpg
Clean:
Top 1: cauliflower (class 938), prob = 1.0000
Top 2: broccoli (class 937), prob = 0.0000
Top 3: head cabbage (class 936), prob = 0.0000
Top 4: custard apple (class 956), prob = 0.0000
Top 5: ear (class 998), prob = 0.0000
Image: picture\菠萝.jpg
Clean:
Top 1: pineapple (class 953), prob = 0.9701
Top 2: banana (class 954), prob = 0.0138
Top 3: orange (class 950), prob = 0.0065
Top 4: strawberry (class 949), prob = 0.0048
Top 5: daisy (class 985), prob = 0.0025
Image: picture\虾.jpg
Clean:
Top 1: isopod (class 126), prob = 0.5244
Top 2: centipede (class 79), prob = 0.3830
Top 3: crayfish (class 124), prob = 0.0228
Top 4: lacewing (class 318), prob = 0.0162
Top 5: sea cucumber (class 329), prob = 0.0131
Image: picture\西红柿.jpg
Clean:
Top 1: bell pepper (class 945), prob = 0.9665
Top 2: cucumber (class 943), prob = 0.0122
Top 3: acorn squash (class 941), prob = 0.0050
Top 4: hip (class 989), prob = 0.0025
Top 5: orange (class 950), prob = 0.0022
Image: picture\车.jpg
Clean:
Top 1: sports car (class 817), prob = 0.3847
Top 2: beach wagon (class 436), prob = 0.3003
Top 3: car wheel (class 479), prob = 0.2519
Top 4: racer (class 751), prob = 0.0379
Top 5: grille (class 581), prob = 0.0084
Image: picture\铅笔.jpg
Clean:
Top 1: pencil box (class 709), prob = 0.4376
Top 2: ballpoint (class 418), prob = 0.2867
Top 3: rubber eraser (class 767), prob = 0.1678
Top 4: pencil sharpener (class 710), prob = 0.0755
Top 5: rule (class 769), prob = 0.0136
Image: picture\青柠.jpg
Clean:
Top 1: lemon (class 951), prob = 0.9673
Top 2: orange (class 950), prob = 0.0189
Top 3: fig (class 952), prob = 0.0072
Top 4: Granny Smith (class 948), prob = 0.0019
Top 5: strawberry (class 949), prob = 0.0009
Image: picture\鸡.jpg
Clean:
Top 1: hen (class 8), prob = 0.6759
Top 2: cock (class 7), prob = 0.3020
Top 3: partridge (class 86), prob = 0.0151
Top 4: ruffed grouse (class 82), prob = 0.0027
Top 5: prairie chicken (class 83), prob = 0.0024
Image: picture\鸡蛋.jpg
Clean:
Top 1: ping-pong ball (class 722), prob = 0.5014
Top 2: orange (class 950), prob = 0.1037
Top 3: maraca (class 641), prob = 0.0984
Top 4: croquet ball (class 522), prob = 0.0736
Top 5: banana (class 954), prob = 0.0282
Image: picture\黄柠檬.jpg
Clean:
Top 1: lemon (class 951), prob = 0.9993
Top 2: orange (class 950), prob = 0.0007
Top 3: banana (class 954), prob = 0.0000
Top 4: pineapple (class 953), prob = 0.0000
Top 5: jackfruit (class 955), prob = 0.0000
Image: picture\乒乓球拍.png
Clean:
Top 1: ping-pong ball (class 722), prob = 0.6082
Top 2: bathing cap (class 433), prob = 0.1226
Top 3: punching bag (class 747), prob = 0.0749
Top 4: spatula (class 813), prob = 0.0435
Top 5: paddle (class 693), prob = 0.0141
Image: picture\电冰箱.png
Clean:
Top 1: sliding door (class 799), prob = 0.8470
Top 2: refrigerator (class 760), prob = 0.0645
Top 3: wardrobe (class 894), prob = 0.0541
Top 4: medicine chest (class 648), prob = 0.0038
Top 5: window screen (class 904), prob = 0.0036
Image: picture\电动车.png
Clean:
Top 1: motor scooter (class 670), prob = 0.7955
Top 2: crash helmet (class 518), prob = 0.0487
Top 3: disk brake (class 535), prob = 0.0389
Top 4: neck brace (class 678), prob = 0.0282
Top 5: moped (class 665), prob = 0.0217
```
### 攻击运行
#### 正常预测
```bash
Image: picture\example.jpg
Clean:
Top 1: giant panda (class 388), prob = 0.9996
Top 2: soccer ball (class 805), prob = 0.0001
Top 3: lesser panda (class 387), prob = 0.0001
Top 4: indri (class 384), prob = 0.0000
Top 5: dalmatian (class 251), prob = 0.0000
```
#### FGSM
```bash
Adversarial (fgsm):
Top 1: giant panda (class 388), prob = 0.9368
Top 2: soccer ball (class 805), prob = 0.0243
Top 3: lesser panda (class 387), prob = 0.0043
Top 4: brown bear (class 294), prob = 0.0012
Top 5: teddy (class 850), prob = 0.0012
```
#### PGD（更强，更容易成功）：
```bash
Adversarial (pgd):
Top 1: soccer ball (class 805), prob = 1.0000
Top 2: volleyball (class 890), prob = 0.0000
Top 3: rugby ball (class 768), prob = 0.0000
Top 4: baseball (class 429), prob = 0.0000
Top 5: football helmet (class 560), prob = 0.0000
```
#### CW-L2（较慢，建议先用较小步数验证）：
```bash
Adversarial (cw):
Top 1: soccer ball (class 805), prob = 0.4585
Top 2: giant panda (class 388), prob = 0.4112
Top 3: teddy (class 850), prob = 0.0054
Top 4: dalmatian (class 251), prob = 0.0037
Top 5: tennis ball (class 852), prob = 0.0033
```
### 防御实验（对比攻击前和攻击后的模型预测结果）：
```bash
python defense_experiments.py --image ./picture/example.jpg --attacks fgsm pgd cw --eps_list 0.015686275 0.031372549
```
#### 模型与数据参数
| 参数             | 你的输入                      | 含义                          |
|----------------|---------------------------|-----------------------------|
| `--model_type` | `standard` (默认)           | 使用标准ResNet50模型（而非对抗训练的鲁棒模型） |
| `--image`      | `./picture/example.jpg`   | 测试的单张图片路径                   |
| `--attacks`    | `fgsm pgd`                | 要执行的攻击方法：FGSM和PGD           |
| `--eps_list`   | `0.015686275 0.031372549` | 扰动半径列表（分别对应 4/255 和 8/255）  |
#### 攻击参数
| 参数           | 默认值     | 含义         |
|--------------|---------|------------|
| `--alpha`    | `2/255` | PGD每步的步长   |
| `--steps`    | `10`    | PGD迭代步数    |
| `--cw_c`     | `1.0`   | CW攻击损失函数权重 |
| `--cw_kappa` | `0.0`   | CW攻击置信度参数  |
| `--cw_steps` | `100`   | CW攻击最大迭代步数 |
| `--cw_lr`    | `0.01`  | CW攻击学习率    |
#### 防御与检测参数
| 参数                     | 默认值     | 含义                       |
|------------------------|---------|--------------------------|
| `--detector_threshold` | `None`  | 特征检测器阈值（None则自动校准）       |
| `--calibrate_dir`      | `None`  | 用于阈值校准的干净样本目录            |
| `--calibrate_n`        | `100`   | 校准样本数量                   |
| `--calibrate_quantile` | `0.99`  | 阈值分位数（0.99 = 取99%分位，较保守） |
| `--use_jpeg`           | `False` | 是否启用JPEG压缩防御（你未启用）       |
| `--jpeg_quality`       | `75`    | JPEG压缩质量                 |
#### 输出详解
| 输出字段                           | 含义                             | 理想值          |
|--------------------------------|--------------------------------|--------------|
| **`attack_success`**           | 攻击成功率：对抗样本是否**成功改变**模型预测？      | 越高说明攻击越强     |
| **`preproc_defense_acc`**      | 预处理防御准确率：对抗样本经防御后是否**恢复**正确预测？ | 越高说明防御越强     |
| **`detector_clean_pass_rate`** | 干净样本通过率：正常样本**未被误检**为对抗样本的比例   | 应接近1.0（降低误报） |
| **`detector_adv_flag_rate`**   | 对抗样本检出率：对抗样本**被正确识别**的比例       | 应接近1.0（提高检出） |
| **`detector_attack_success`**  | 攻击成功且**未被检测**的比例（绕过率）          | 越低越好         |


### 对普通模型
```bash
attack=fgsm, eps=0.01569, attack_success=0.500, preproc_defense_acc=0.740, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.020, detector_attack_success=0.500
attack=fgsm, eps=0.03137, attack_success=0.520, preproc_defense_acc=0.600, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.000, detector_attack_success=0.520
attack=pgd, eps=0.01569, attack_success=0.920, preproc_defense_acc=0.740, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.200, detector_attack_success=0.720
attack=pgd, eps=0.03137, attack_success=1.000, preproc_defense_acc=0.620, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.520, detector_attack_success=0.480
attack=cw, eps=0.01569, attack_success=1.000, preproc_defense_acc=0.840, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.020, detector_attack_success=0.980
attack=cw, eps=0.03137, attack_success=1.000, preproc_defense_acc=0.840, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.020, detector_attack_success=0.980
attack=cw, eps=0.06275, attack_success=1.000, preproc_defense_acc=0.840, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.020, detector_attack_success=0.980
```

### 强健模型
```bash
attack=fgsm, eps=0.01569, attack_success=0.420, preproc_defense_acc=0.500, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.000, detector_attack_success=0.420
attack=fgsm, eps=0.03137, attack_success=0.620, preproc_defense_acc=0.340, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.000, detector_attack_success=0.620
attack=pgd, eps=0.01569, attack_success=0.540, preproc_defense_acc=0.480, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.000, detector_attack_success=0.540
attack=pgd, eps=0.03137, attack_success=0.800, preproc_defense_acc=0.320, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.000, detector_attack_success=0.800
attack=cw, eps=0.01569, attack_success=0.280, preproc_defense_acc=0.700, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.020, detector_attack_success=0.260
attack=cw, eps=0.03137, attack_success=0.280, preproc_defense_acc=0.700, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.020, detector_attack_success=0.260
attack=cw, eps=0.06275, attack_success=0.280, preproc_defense_acc=0.700, detector_clean_pass_rate=0.980, detector_adv_flag_rate=0.020, detector_attack_success=0.260
```

### 效果分析
在本实验中，我们分别在普通模型与强健模型上，采用 FGSM 与 PGD 两种典型基于梯度的对抗攻击方法，在不同扰动强度（ε = 0.01569 与 ε = 0.03137）的设置下评估了模型在无防御、仅采用预处理防御以及联合预处理与检测机制时的鲁棒性表现。总体来看，在相同扰动幅度下，PGD 攻击的破坏能力显著强于 FGSM：在普通模型上，当 ε = 0.01569 时，FGSM 的攻击成功率为 0.500，而 PGD 的攻击成功率高达 0.920；当 ε 增大至 0.03137 时，PGD 攻击几乎可以实现 100% 的成功率，反映出迭代式攻击相较于单步攻击对模型具有更强的威胁。

对于普通模型，从防御角度看，预处理防御在中等扰动强度下对分类性能具有一定修复能力：在 ε = 0.01569 时，无论针对 FGSM 还是 PGD，对抗样本在经过预处理后模型准确率均可恢复到 0.740，说明该类防御在较小扰动范围内能够有效缓解对抗噪声的影响。然而，当 ε 增大到 0.03137 时，预处理防御的效果明显退化，准确率分别降至 0.600（FGSM）和 0.620（PGD），表明该方法对大幅度扰动的鲁棒性有限。对抗样本检测模块在保持低误报率方面表现稳定：在所有设置下，检测器对干净样本的通过率均为 0.980，说明其对正常样本的干扰较小。但在召回对抗样本方面，不同攻击与扰动强度之间存在明显差异。对于 FGSM 攻击，检测器几乎无法有效识别对抗样本，在两档 ε 下的对抗样本标记率仅为 0.020 和 0.000；相比之下，对于 PGD 攻击，检测器在 ε = 0.01569 时能标记约 20% 的对抗样本，在 ε = 0.03137 时标记率提升至 0.520，说明随着扰动强度的增大，对抗样本在特征空间中与正常样本的分布差异更易被检测器捕捉，从而带来一定检测收益。综合预处理与检测两种防御机制的联合作用来看，在强攻击场景下防御仍然存在明显不足：以 PGD+大 ε 为例，虽然检测器能够识别超过一半的对抗样本，并辅以预处理防御将部分样本恢复为正确分类，但最终攻击成功率仍为 0.480，仅将原始 100% 的攻击成功率削弱一半左右；在 PGD+小 ε 场景下，联合防御后的攻击成功率依旧高达 0.720，模型鲁棒性仍然较为脆弱。

在强健模型上，基础鲁棒性相较普通模型整体有所提升，尤其体现在迭代式 PGD 攻击下：当 ε = 0.01569 时，PGD 的攻击成功率由普通模型上的 0.920 降至 0.540；当 ε = 0.03137 时，从 1.000 降至 0.800，表明强健模型在面对较强攻击时能够显著压制部分对抗样本的成功率。对于 FGSM，小扰动 ε = 0.01569 下的攻击成功率从 0.500 降至 0.420，同样体现出一定鲁棒性提升；但在较大扰动 ε = 0.03137 时，FGSM 的攻击成功率反而从 0.520 升至 0.620，说明强健模型在某些方向上仍存在易受单步大幅度扰动影响的脆弱性。

值得注意的是，在强健模型上，沿用同一预处理防御策略的收益明显弱于普通模型：在四种攻击设置下，对抗样本经预处理后的准确率仅为 0.500、0.340（FGSM）以及 0.480、0.320（PGD），均显著低于普通模型对应的 0.740/0.600 与 0.740/0.620。这表明预处理模块与强健模型所学习到的对抗鲁棒特征分布并不完全匹配，其在普通模型上表现出的“纠偏”能力并不能直接迁移到强健模型上。另一方面，检测器在强健模型上的表现也发生了明显退化：虽然对干净样本的通过率仍保持在 0.980，但在所有攻击与扰动强度设置下，对抗样本标记率均为 0.000，即检测器几乎无法在强健模型的特征空间中区分正常样本与对抗样本。结合结果可以看出，在强健模型场景下，预处理与检测模块几乎未能进一步降低攻击成功率，对强健模型的增益十分有限。

在此基础上，我们进一步考察了优化式 CW 攻击在两类模型上的表现。对于普通模型，在 ε = 0.01569、0.03137 与 0.06275 三档扰动强度下，CW 攻击在无防御情形下的攻击成功率均为 1.000，表明该攻击能够在极高概率上找到有效对抗样本。即便引入预处理防御后，对抗样本的分类准确率也仅能提升至约 0.840，且在三档 ε 下基本保持不变，说明 CW 生成的扰动更具针对性，预处理难以显著削弱其破坏效果。与此同时，检测器对干净样本的通过率仍稳定在 0.980，但对 CW 对抗样本的标记率仅约为 0.020，最终攻击成功率仍高达 0.980，整体防御体系对 CW 攻击几乎无实质抑制作用。

与此形成对比的是，在强健模型上，CW 攻击的基础成功率显著降低：在相同三档 ε 下，攻击成功率稳定在 0.280，表明对抗训练等鲁棒化手段能够有效压制优化式攻击的成功率。对抗样本经预处理后，模型准确率进一步提升至 0.700，显示出预处理在强健模型的特征空间中仍具有一定纠偏能力。检测器在该场景下对干净样本的通过率依旧为 0.980，对 CW 对抗样本的标记率约为 0.020，使最终攻击成功率略微下降至 0.260。整体而言，强健模型在 CW 攻击下展现出比普通模型显著更高的鲁棒性，但现有检测器仅能带来极其有限的额外收益。

综合普通模型与强健模型的两组实验结果可以得到如下结论：
1. 在相同扰动约束下，PGD 等迭代式攻击以及 CW 等优化式攻击始终比单步 FGSM 具有更强的攻击能力，是评估模型鲁棒性的更严格基准；
2. 对于普通模型，预处理防御在中等幅度对抗扰动下具有一定缓解作用，但其防御效果随扰动强度增大快速下降；检测器在保证低误报的同时，对弱扰动和 FGSM 类型对抗样本的识别能力不足，仅在强 PGD 攻击场景下表现出有限的检测效果；
3. 引入强健模型可以显著降低在 PGD 等强攻击下的基础攻击成功率，但同时也削弱了原有预处理与检测模块的增益，表明不同防御组件之间存在明显的相互作用与适配问题；
4. 整体而言，单一或简单串联的预处理与检测机制难以在强攻击场景下将攻击成功率压制到满意水平，后续需要结合对抗训练、联合优化检测器与分类器等更系统性的防御策略，以在保持低误报的同时进一步提升模型的整体对抗鲁棒性。

## 迁移攻击
```bash
python blackbox_transfer.py --image_dir picture --attacks fgsm pgd cw --eps 0.031372549 --alpha 0.007843137 --steps 10 --cw_c 1.0 --cw_kappa 0.0 --cw_steps 50 --cw_lr 0.01 --visualize_n 10
Attack/Model    VGG19   ViT     Swin
FGSM    38.0%   14.0%   26.0%
PGD     46.0%   10.0%   36.0%
CW      2.0%    2.0%    8.0%
```
### 结果分析
**结果整体解读**

表中各百分数反映了在相同实验设置下，不同攻击方法对三种模型架构的破坏效果。可以观察到：
- 对于 **VGG19**，FGSM 和 PGD 的攻击成功率分别为 38.0% 与 46.0%，明显高于 CW 的 2.0%，说明传统卷积网络在本实验设置下对基于梯度的一步与多步攻击（FGSM/PGD）更为脆弱，而对 CW 攻击表现出较强的鲁棒性或极低的迁移性。
- 对于 **ViT**，FGSM 与 PGD 的攻击成功率分别为 14.0% 和 10.0%，均显著低于 VGG19，对 CW 的攻击成功率仅为 2.0%。整体来看，ViT 在三种攻击下均取得了最低的攻击成功率，体现出相对更好的对抗鲁棒性，这与近年来一些工作中关于 Transformer 架构在损失景观平滑性和全局建模能力方面的优势相吻合。
- 对于 **Swin**，FGSM 与 PGD 的攻击成功率分别为 26.0% 与 36.0%，介于 VGG19 和 ViT 之间，而 CW 的攻击成功率为 8.0%，高于 ViT 但仍明显低于 FGSM/PGD。说明层级化的 Transformer 结构在对抗鲁棒性上处于“折中”位置：相较传统卷积网络有所提升，但整体仍不及标准 ViT。

从攻击方法的维度看：
- **PGD 行**（46.0%、10.0%、36.0%）整体上对三种模型的破坏力高于或接近 FGSM 行（38.0%、14.0%、26.0%），符合迭代式攻击相较单步攻击更强的普遍结论。
- **CW 行** 的数值（2.0%、2.0%、8.0%）在三种模型上均远低于 FGSM/PGD，对普通卷积网络与 Transformer 的攻击成功率均处于极低水平。这一现象结合你前面基于单模型的实验结果，可以合理解释为：在白盒场景下，CW 攻击可以达到极高的成功率，但在当前表格所对应的场景（更可能是跨模型迁移或不同架构下的泛化攻击）中，其**迁移性较差**，导致在其他模型上观测到的攻击成功率显著衰减。

**可用于报告的总结性表述示例**
在对三种代表性模型架构（VGG19、ViT 与 Swin Transformer）进行横向对比时，可以进一步观察到基于卷积与基于 Transformer 的网络在对抗鲁棒性上的系统性差异。总体而言，VGG19 在 FGSM 与 PGD 攻击下的攻击成功率分别达到 38.0% 与 46.0%，明显高于 ViT 与 Swin，对应地反映出传统卷积网络在本实验设置下对基于梯度的一步与多步攻击更加敏感。相比之下，ViT 在三种攻击下的攻击成功率分别仅为 14.0%、10.0% 和 2.0%，在所有配置中均取得最低值，表明其全局自注意力机制和更平滑的特征表示在一定程度上提升了对抗鲁棒性；Swin 的表现介于二者之间（如 FGSM 与 PGD 下攻击成功率为 26.0% 与 36.0%），呈现出“折中型”的鲁棒性水平。

从攻击方法的角度来看，PGD 在各模型上的攻击成功率普遍不低于 FGSM，印证了迭代式攻击普遍强于单步攻击的观察。而 CW 攻击在三种模型上的攻击成功率仅为 2.0%–8.0%，显著低于 FGSM 与 PGD。这一结果与前文在单模型白盒设定中 CW 攻击几乎可以实现 100% 成功率的现象形成鲜明对比，说明在本表所对应的评测设定中（例如跨模型迁移或异构架构间的泛化攻击），CW 生成的对抗样本**迁移性有限**，在其他模型上难以保持同等强度的破坏效果。因此，从综合安全性角度出发，PGD 及类似的迭代式攻击更适合作为评估模型跨架构鲁棒性的“强基准”，而 CW 攻击在迁移场景中的威胁则相对较弱。
